import xarray as xr
from shapely.geometry import Point, Polygon
from app.schemas import AQIData, AQIResponse, AQIDataHourly
from app.thailand_polygon_detailed import THAILAND_POLYGON_DETAILED
import httpx
from datetime import datetime, timedelta
import numpy as np
import os
import requests
import pickle
import logging
from dotenv import load_dotenv
from typing import List
import random


from app.helper import *

client = httpx.Client(
    base_url="https://air-quality-api.open-meteo.com/v1",
    timeout=10.0,
    limits=httpx.Limits(max_connections=10, max_keepalive_connections=5)
)

with open('model.pkl', 'rb') as f:
    model = pickle.load(f)

def get_aqi_status(aqi: float) -> str:
    if aqi <= 50:
        return "Good"
    elif aqi <= 100:
        return "Moderate"
    elif aqi <= 150:
        return "Unhealthy for Sensitive Groups"
    elif aqi <= 200:
        return "Unhealthy"
    elif aqi <= 300:
        return "Very Unhealthy"
    else:
        return "Hazardous"

def frange(start: float, stop: float, step: float):
    while start <= stop:
        yield round(start, 5)
        start += step

# –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π –ø–æ–ª–∏–≥–æ–Ω –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞ (–¥–ª—è –ø—Ä–∏–º–µ—Ä–∞; –ª—É—á—à–µ –ø–æ–¥—Å—Ç–∞–≤–∏—Ç—å –ø–æ–ª–Ω—ã–π –∏–∑ GeoJSON)
from shapely.geometry import Polygon, Point


import random
from shapely.geometry import Point

# Pre-generate sample points only once (at module load or on first call)
def generate_random_points_in_polygon(polygon, n_points=40, max_attempts=10000):
    min_lon, min_lat, max_lon, max_lat = polygon.bounds
    pts = []
    attempts = 0
    while len(pts) < n_points and attempts < max_attempts:
        rand_lon = random.uniform(min_lon, max_lon)
        rand_lat = random.uniform(min_lat, max_lat)
        p = Point(rand_lon, rand_lat)
        if polygon.contains(p):
            pts.append(p)
        attempts += 1
    return pts

# Suppose THAILAND_POLYGON_DETAILED is defined already
SAMPLE_POINTS = generate_random_points_in_polygon(THAILAND_POLYGON_DETAILED, n_points=40)

def fetch_kazakhstan_air_quality() -> AQIResponse:
    """
    –ü–æ–ª—É—á–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –≤–æ–∑–¥—É—Ö–∞ –ø–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Å–ª—É—á–∞–π–Ω—ã–º —Ç–æ—á–∫–∞–º –≤–Ω—É—Ç—Ä–∏ –ø–æ–ª–∏–≥–æ–Ω–∞ –¢–∞–∏–ª–∞–Ω–¥–∞.
    """
    result = []
    for p in SAMPLE_POINTS:
        rand_lon = p.x
        rand_lat = p.y

        params = {
            "latitude": rand_lat,
            "longitude": rand_lon,
            "current": [
                "european_aqi",
                "sulphur_dioxide",
                "pm10",
                "pm2_5",
                "carbon_monoxide",
                "nitrogen_dioxide",
            ],
        }
        r = client.get("/air-quality", params=params)
        if r.status_code != 200:
            continue
        data = r.json()
        if "current" in data:
            current_aqi = data["current"]["european_aqi"]
            result.append(AQIData(
                latitude=data["latitude"],
                longitude=data["longitude"],
                aqi=current_aqi,
                pm10=data["current"].get("pm10"),
                pm2_5=data["current"].get("pm2_5"),
                co=data["current"].get("carbon_monoxide"),
                no2=data["current"].get("nitrogen_dioxide"),
                so2=data["current"].get("sulphur_dioxide"),
                status=get_aqi_status(current_aqi),
            ))
    return AQIResponse(data=result)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S",
)
log = logging.getLogger("NASA_AQI")

# === –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∞ ===
load_dotenv()
NASA_TOKEN = os.getenv("NASA_TOKEN")
if not NASA_TOKEN:
    log.error("‚ùå NASA_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
HEADERS = {"Authorization": f"Bearer {NASA_TOKEN}"}

def fetch_latest_nc_link(short_name: str) -> str:
    """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π .nc —Ñ–∞–π–ª –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ short_name"""
    try:
        url = "https://cmr.earthdata.nasa.gov/search/granules.json"
        params = {"short_name": short_name, "page_size": 1, "sort_key": "-start_date"}
        res = requests.get(url, headers=HEADERS, params=params)
        res.raise_for_status()

        data = res.json()
        entries = data.get("feed", {}).get("entry", [])
        if not entries:
            log.warning(f"üö´ –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –¥–ª—è {short_name}")
            return None

        links = entries[0].get("links", [])
        nc = next((l["href"] for l in links if l["href"].endswith(".nc")), None)

        if nc:
            log.info(f"‚úÖ –ù–∞–π–¥–µ–Ω .nc —Ñ–∞–π–ª –¥–ª—è {short_name}: {nc}")
        else:
            log.warning(f"‚ö†Ô∏è –î–ª—è {short_name} –Ω–µ—Ç —Å—Å—ã–ª–æ–∫ –Ω–∞ .nc")
        return nc

    except Exception as e:
        log.exception(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {short_name}: {e}")
        return None

def open_tempo_file(nc_url: str) -> xr.Dataset:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç NetCDF —Ñ–∞–π–ª —Å –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""
    if not nc_url:
        return None

    filename = os.path.basename(nc_url)
    if not os.path.exists(filename):
        log.info(f"‚¨áÔ∏è –°–∫–∞—á–∏–≤–∞—é {filename}")
        try:
            res = requests.get(nc_url, headers=HEADERS)
            res.raise_for_status()
            with open(filename, "wb") as f:
                f.write(res.content)
            log.info(f"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {filename}")
        except Exception as e:
            log.exception(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {filename}: {e}")
            return None

    try:
        ds = xr.open_dataset(filename, engine="netcdf4")
        log.info(f"üìÇ –§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –æ—Ç–∫—Ä—ã—Ç: {filename}")
        return ds
    except Exception as e:
        log.exception(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–∫—Ä—ã—Ç–∏—è {filename}: {e}")
        return None

def find_concentration_var(ds: xr.Dataset, pollutant: str) -> str:
    """–ò—â–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏"""
    candidates = [
        v for v in ds.data_vars
        if any(k in v.lower() for k in [pollutant, "column", "conc", "vmr", "amount"])
    ]
    if candidates:
        log.info(f"üîç –î–ª—è {pollutant.upper()} –Ω–∞–π–¥–µ–Ω–æ –ø–æ–ª–µ: {candidates[0]}")
        return candidates[0]
    else:
        log.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –ø–æ–ª–µ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è {pollutant}")
        return None

def fetch_global_aqi() -> List[dict]:
    datasets = {
        "no2": "TEMPO_NO2_L2_NRT",
        "so2": "TEMPO_SO2_L2_NRT",
        "co": "TEMPO_CO_L2_NRT",
        "pm2_5": "TEMPO_PM25_L2_NRT",
        "pm10": "TEMPO_PM10_L2_NRT",
    }

    data = {}
    coords = None

    for pol, short_name in datasets.items():
        log.info(f"\nüåç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {pol.upper()} ...")
        nc_link = fetch_latest_nc_link(short_name)
        if not nc_link:
            continue

        ds = open_tempo_file(nc_link)
        if ds is None:
            continue

        var = find_concentration_var(ds, pol)
        if not var:
            ds.close()
            continue

        try:
            # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–ª–æ–π, –µ—Å–ª–∏ –µ—Å—Ç—å
            if "time" in ds[var].dims:
                arr = ds[var].isel(time=0).values
                log.info(f"üïê –í—ã–±—Ä–∞–Ω –ø–µ—Ä–≤—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–ª–æ–π –¥–ª—è {pol.upper()}")
            else:
                arr = ds[var].values
        except Exception as e:
            log.exception(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö {pol}: {e}")
            ds.close()
            continue

        lat_name = next((n for n in ["lat", "latitude", "Latitude"] if n in ds), None)
        lon_name = next((n for n in ["lon", "longitude", "Longitude"] if n in ds), None)

        if lat_name and lon_name:
            coords = (ds[lat_name].values, ds[lon_name].values)
            log.info(f"üó∫Ô∏è –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –Ω–∞–π–¥–µ–Ω—ã ({lat_name}, {lon_name})")

        log.info(f"‚úÖ {pol.upper()} –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã: shape={arr.shape}")
        data[pol] = arr
        ds.close()

    if not data:
        log.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–≥—Ä—è–∑–Ω–∏—Ç–µ–ª—è")
        raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è AQI")

    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤
    shapes = [v.shape for v in data.values()]
    min_shape = tuple(np.min(shapes, axis=0))
    if len(set(shapes)) > 1:
        log.warning(f"‚ö†Ô∏è –†–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã —Å–µ—Ç–æ–∫ {shapes}, –æ–±—Ä–µ–∑–∞–µ–º –¥–æ {min_shape}")

    for k in data:
        if data[k].shape != min_shape:
            data[k] = data[k][:min_shape[0], :min_shape[1]]

    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä–∞—Å—á—ë—Ç AQI
    log.info("‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω—ã, –≥–æ—Ç–æ–≤–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç...")

    lats, lons = coords if coords else (np.arange(min_shape[0]), np.arange(min_shape[1]))
    results = []

    for i, lat in enumerate(lats):
        for j, lon in enumerate(lons):
            results.append({
                "lat": float(lat),
                "lon": float(lon),
                **{p: float(data[p][i, j]) if p in data else None for p in data.keys()}
            })

    log.info(f"üèÅ –ì–æ—Ç–æ–≤–æ! –í—Å–µ–≥–æ —Ç–æ—á–µ–∫: {len(results)}")
    return results
    
def fetch_air_quality(latitude, longitude) -> AQIDataHourly:
    today = datetime.today().date()
    one_month_before = today - timedelta(days=14)

    params = {
        "latitude": latitude,
        "longitude": longitude,
        "current": "european_aqi",
        "hourly": ["pm10", "pm2_5", "carbon_monoxide", "nitrogen_dioxide", "sulphur_dioxide", "ozone"],
        "start_date": one_month_before.strftime("%Y-%m-%d"),
        "end_date": today.strftime("%Y-%m-%d"),
    }

    r = client.get("/air-quality", params=params)
    r.raise_for_status()
    data = r.json()

    current_aqi = data["current"]["european_aqi"]

    # --- —Ä–∞—Å—á—ë—Ç –ø–æ—á–∞—Å–æ–≤–æ–≥–æ AQI ---
    n = len(data["hourly"]["pm2_5"])
    hourly_aqi = []
    for i in range(n):
        vals = {
            "pm2_5": data["hourly"]["pm2_5"][i],
            "pm10": data["hourly"]["pm10"][i],
            "carbon_monoxide": data["hourly"]["carbon_monoxide"][i],
            "nitrogen_dioxide": data["hourly"]["nitrogen_dioxide"][i],
            "sulphur_dioxide": data["hourly"]["sulphur_dioxide"][i],
            "ozone": data["hourly"]["ozone"][i],
        }
        # —Ä–∞—Å—á—ë—Ç AQI –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–≥—Ä—è–∑–Ω–∏—Ç–µ–ª—è
        aqi_values = []
        for k, v in vals.items():
            aqi_val = calc_aqi(v, BREAKPOINTS[k])
            if aqi_val is not None:
                aqi_values.append(aqi_val)

        hourly_aqi.append(max(aqi_values) if aqi_values else None)

    result = AQIDataHourly(
        latitude=data["latitude"],
        longitude=data["longitude"],
        aqi=current_aqi,
        aqi_hourly=hourly_aqi,
        status=get_aqi_status(current_aqi),
        pm10=data["hourly"]["pm10"],
        pm2_5=data["hourly"]["pm2_5"],
        co=data["hourly"]["carbon_monoxide"],
        no2=data["hourly"]["nitrogen_dioxide"],
        so2=data["hourly"]["sulphur_dioxide"],
        o3=data["hourly"]["ozone"],
    )
    return result

def predict_health_impact(aqi: float, pm10: float, pm2_5: float, no2: float, so2: float, o3: float) -> dict:
    """
    –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –∑–¥–æ—Ä–æ–≤—å–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    """
    try:
        input_data = np.array([[aqi, pm10, pm2_5, no2, so2, o3]])
        prediction = model.predict(input_data)
        result = float(prediction[0])  # üîπ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π float
        return {"prediction": result}
    except Exception as e:
        log.exception(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –≤–ª–∏—è–Ω–∏—è –Ω–∞ –∑–¥–æ—Ä–æ–≤—å–µ: {e}")
        return {"error": "Prediction failed"}