import xarray as xr
from shapely.geometry import Point, Polygon
from app.schemas import AQIData, AQIResponse, AQIDataHourly
from fastapi.encoders import jsonable_encoder
import httpx
from datetime import datetime, timedelta
import numpy as np
import os


from app.helper import *

client = httpx.Client(
    base_url="https://air-quality-api.open-meteo.com/v1",
    timeout=10.0,
    limits=httpx.Limits(max_connections=10, max_keepalive_connections=5)
)

with open('model.pkl', 'rb') as f:
    model = pickle.load(f)

def get_aqi_status(aqi: float) -> str:
    if aqi <= 50:
        return "Good"
    elif aqi <= 100:
        return "Moderate"
    elif aqi <= 150:
        return "Unhealthy for Sensitive Groups"
    elif aqi <= 200:
        return "Unhealthy"
    elif aqi <= 300:
        return "Very Unhealthy"
    else:
        return "Hazardous"

def frange(start: float, stop: float, step: float):
    while start <= stop:
        yield round(start, 5)
        start += step

# –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π –ø–æ–ª–∏–≥–æ–Ω –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞ (–¥–ª—è –ø—Ä–∏–º–µ—Ä–∞; –ª—É—á—à–µ –ø–æ–¥—Å—Ç–∞–≤–∏—Ç—å –ø–æ–ª–Ω—ã–π –∏–∑ GeoJSON)
from shapely.geometry import Polygon, Point

# –ü–æ–ª–∏–≥–æ–Ω –ë–µ–ª—å–≥–∏–∏ (–ø—Ä–∏–±–ª–∏–∂—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç—É—Ä)
BELGIUM_POLYGON = Polygon([
    (2.51357303225, 51.1485061713),   # —Å–µ–≤–µ—Ä–æ-–∑–∞–ø–∞–¥ (–≥—Ä–∞–Ω–∏—Ü–∞ —Å –§—Ä–∞–Ω—Ü–∏–µ–π –∏ –°–µ–≤–µ—Ä–Ω—ã–º –º–æ—Ä–µ–º)
    (3.31497114423, 51.3457809515),
    (4.04707116051, 51.2672586127),
    (4.97399132662, 51.4750237087),
    (5.60697594567, 51.03729848897),  # —Å–µ–≤–µ—Ä–æ-–≤–æ—Å—Ç–æ–∫ (–≥—Ä–∞–Ω–∏—Ü–∞ —Å –ù–∏–¥–µ—Ä–ª–∞–Ω–¥–∞–º–∏/–ì–µ—Ä–º–∞–Ω–∏–µ–π)
    (6.15665815596, 50.803721015),
    (6.04307335778, 50.1280516628),
    (5.7824174333, 50.0903278672),
    (5.67405195478, 49.5294835476),   # —é–≥–æ-–≤–æ—Å—Ç–æ–∫ (–≥—Ä–∞–Ω–∏—Ü–∞ —Å –õ—é–∫—Å–µ–º–±—É—Ä–≥–æ–º)
    (4.79922163252, 49.9853730332),
    (4.28602298343, 49.9074966498),
    (3.58818444176, 50.3789924180),
    (2.56870071544, 50.4020917622),
    (2.51357303225, 51.1485061713)    # –∑–∞–º—ã–∫–∞–Ω–∏–µ –ø–æ–ª–∏–≥–æ–Ω–∞
])

def fetch_kazakhstan_air_quality(step: float = 0.5) -> AQIResponse:
    """
    –ü–æ–ª—É—á–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –≤–æ–∑–¥—É—Ö–∞ –≤–Ω—É—Ç—Ä–∏ –ø–æ–ª–∏–≥–æ–Ω–∞ –ë–µ–ª—å–≥–∏–∏.
    step - —à–∞–≥ —Å–µ—Ç–∫–∏ (–≤ –≥—Ä–∞–¥—É—Å–∞—Ö).
    """
    lat_start, lat_end = 49.5, 51.5
    lon_start, lon_end = 2.5, 6.5

    result = []
    for lat in frange(lat_start, lat_end, step):
        for lon in frange(lon_start, lon_end, step):
            point = Point(lon, lat)  # shapely: (x=lon, y=lat)
            if not BELGIUM_POLYGON.contains(point):
                continue

            params = {
                "latitude": lat,
                "longitude": lon,
                "current": [
                    "european_aqi",
                    "sulphur_dioxide",
                    "pm10",
                    "pm2_5",
                    "carbon_monoxide",
                    "nitrogen_dioxide",
                ],
            }
            r = client.get("/air-quality", params=params)
            if r.status_code != 200:
                continue

            data = r.json()
            if "current" in data:
                current_aqi = data["current"]["european_aqi"]
                result.append(AQIData(
                    latitude=data["latitude"],
                    longitude=data["longitude"],
                    aqi=current_aqi,
                    pm10=data["current"].get("pm10"),
                    pm2_5=data["current"].get("pm2_5"),
                    co=data["current"].get("carbon_monoxide"),
                    no2=data["current"].get("nitrogen_dioxide"),
                    so2=data["current"].get("sulphur_dioxide"),
                    status=get_aqi_status(current_aqi),
                ))

    return AQIResponse(data=result)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S",
)
log = logging.getLogger("NASA_AQI")

# === –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∞ ===
load_dotenv()
NASA_TOKEN = os.getenv("NASA_TOKEN")
if not NASA_TOKEN:
    log.error("‚ùå NASA_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
HEADERS = {"Authorization": f"Bearer {NASA_TOKEN}"}


def fetch_latest_nc_link(short_name: str) -> str:
    """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π .nc —Ñ–∞–π–ª –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ short_name"""
    try:
        url = "https://cmr.earthdata.nasa.gov/search/granules.json"
        params = {"short_name": short_name, "page_size": 1, "sort_key": "-start_date"}
        res = requests.get(url, headers=HEADERS, params=params)
        res.raise_for_status()

        data = res.json()
        entries = data.get("feed", {}).get("entry", [])
        if not entries:
            log.warning(f"üö´ –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –¥–ª—è {short_name}")
            return None

        links = entries[0].get("links", [])
        nc = next((l["href"] for l in links if l["href"].endswith(".nc")), None)

        if nc:
            log.info(f"‚úÖ –ù–∞–π–¥–µ–Ω .nc —Ñ–∞–π–ª –¥–ª—è {short_name}: {nc}")
        else:
            log.warning(f"‚ö†Ô∏è –î–ª—è {short_name} –Ω–µ—Ç —Å—Å—ã–ª–æ–∫ –Ω–∞ .nc")
        return nc

    except Exception as e:
        log.exception(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {short_name}: {e}")
        return None


def open_tempo_file(nc_url: str) -> xr.Dataset:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç NetCDF —Ñ–∞–π–ª —Å –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""
    if not nc_url:
        return None

    filename = os.path.basename(nc_url)
    if not os.path.exists(filename):
        log.info(f"‚¨áÔ∏è –°–∫–∞—á–∏–≤–∞—é {filename}")
        try:
            res = requests.get(nc_url, headers=HEADERS)
            res.raise_for_status()
            with open(filename, "wb") as f:
                f.write(res.content)
            log.info(f"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {filename}")
        except Exception as e:
            log.exception(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {filename}: {e}")
            return None

    try:
        ds = xr.open_dataset(filename, engine="netcdf4")
        log.info(f"üìÇ –§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –æ—Ç–∫—Ä—ã—Ç: {filename}")
        return ds
    except Exception as e:
        log.exception(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–∫—Ä—ã—Ç–∏—è {filename}: {e}")
        return None


def find_concentration_var(ds: xr.Dataset, pollutant: str) -> str:
    """–ò—â–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏"""
    candidates = [
        v for v in ds.data_vars
        if any(k in v.lower() for k in [pollutant, "column", "conc", "vmr", "amount"])
    ]
    if candidates:
        log.info(f"üîç –î–ª—è {pollutant.upper()} –Ω–∞–π–¥–µ–Ω–æ –ø–æ–ª–µ: {candidates[0]}")
        return candidates[0]
    else:
        log.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –ø–æ–ª–µ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è {pollutant}")
        return None


def fetch_global_aqi() -> List[dict]:
    datasets = {
        "no2": "TEMPO_NO2_L2_NRT",
        "so2": "TEMPO_SO2_L2_NRT",
        "co": "TEMPO_CO_L2_NRT",
        "pm2_5": "TEMPO_PM25_L2_NRT",
        "pm10": "TEMPO_PM10_L2_NRT",
    }

    data = {}
    coords = None

    for pol, short_name in datasets.items():
        log.info(f"\nüåç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {pol.upper()} ...")
        nc_link = fetch_latest_nc_link(short_name)
        if not nc_link:
            continue

        ds = open_tempo_file(nc_link)
        if ds is None:
            continue

        var = find_concentration_var(ds, pol)
        if not var:
            ds.close()
            continue

        try:
            # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–ª–æ–π, –µ—Å–ª–∏ –µ—Å—Ç—å
            if "time" in ds[var].dims:
                arr = ds[var].isel(time=0).values
                log.info(f"üïê –í—ã–±—Ä–∞–Ω –ø–µ—Ä–≤—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–ª–æ–π –¥–ª—è {pol.upper()}")
            else:
                arr = ds[var].values
        except Exception as e:
            log.exception(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö {pol}: {e}")
            ds.close()
            continue

        lat_name = next((n for n in ["lat", "latitude", "Latitude"] if n in ds), None)
        lon_name = next((n for n in ["lon", "longitude", "Longitude"] if n in ds), None)

        if lat_name and lon_name:
            coords = (ds[lat_name].values, ds[lon_name].values)
            log.info(f"üó∫Ô∏è –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –Ω–∞–π–¥–µ–Ω—ã ({lat_name}, {lon_name})")

        log.info(f"‚úÖ {pol.upper()} –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã: shape={arr.shape}")
        data[pol] = arr
        ds.close()

    if not data:
        log.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–≥—Ä—è–∑–Ω–∏—Ç–µ–ª—è")
        raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è AQI")

    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤
    shapes = [v.shape for v in data.values()]
    min_shape = tuple(np.min(shapes, axis=0))
    if len(set(shapes)) > 1:
        log.warning(f"‚ö†Ô∏è –†–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã —Å–µ—Ç–æ–∫ {shapes}, –æ–±—Ä–µ–∑–∞–µ–º –¥–æ {min_shape}")

    for k in data:
        if data[k].shape != min_shape:
            data[k] = data[k][:min_shape[0], :min_shape[1]]

    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä–∞—Å—á—ë—Ç AQI
    log.info("‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω—ã, –≥–æ—Ç–æ–≤–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç...")

    lats, lons = coords if coords else (np.arange(min_shape[0]), np.arange(min_shape[1]))
    results = []

    for i, lat in enumerate(lats):
        for j, lon in enumerate(lons):
            results.append({
                "lat": float(lat),
                "lon": float(lon),
                **{p: float(data[p][i, j]) if p in data else None for p in data.keys()}
            })

    log.info(f"üèÅ –ì–æ—Ç–æ–≤–æ! –í—Å–µ–≥–æ —Ç–æ—á–µ–∫: {len(results)}")
    return results
    

def fetch_air_quality(latitude, longitude) -> AQIDataHourly:
    today = datetime.today().date()
    one_month_before = today - timedelta(days=14)

    params = {
        "latitude": latitude,
        "longitude": longitude,
        "current": "european_aqi",
        "hourly": ["pm10", "pm2_5", "carbon_monoxide", "nitrogen_dioxide", "sulphur_dioxide", "ozone"],
        "start_date": one_month_before.strftime("%Y-%m-%d"),
        "end_date": today.strftime("%Y-%m-%d"),
    }

    r = client.get("/air-quality", params=params)
    r.raise_for_status()
    data = r.json()

    current_aqi = data["current"]["european_aqi"]

    # --- —Ä–∞—Å—á—ë—Ç –ø–æ—á–∞—Å–æ–≤–æ–≥–æ AQI ---
    n = len(data["hourly"]["pm2_5"])
    hourly_aqi = []
    for i in range(n):
        vals = {
            "pm2_5": data["hourly"]["pm2_5"][i],
            "pm10": data["hourly"]["pm10"][i],
            "carbon_monoxide": data["hourly"]["carbon_monoxide"][i],
            "nitrogen_dioxide": data["hourly"]["nitrogen_dioxide"][i],
            "sulphur_dioxide": data["hourly"]["sulphur_dioxide"][i],
            "ozone": data["hourly"]["ozone"][i],
        }
        # —Ä–∞—Å—á—ë—Ç AQI –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–≥—Ä—è–∑–Ω–∏—Ç–µ–ª—è
        aqi_values = []
        for k, v in vals.items():
            aqi_val = calc_aqi(v, BREAKPOINTS[k])
            if aqi_val is not None:
                aqi_values.append(aqi_val)

        hourly_aqi.append(max(aqi_values) if aqi_values else None)

    result = AQIDataHourly(
        latitude=data["latitude"],
        longitude=data["longitude"],
        aqi=current_aqi,
        aqi_hourly=hourly_aqi,
        status=get_aqi_status(current_aqi),
        pm10=data["hourly"]["pm10"],
        pm2_5=data["hourly"]["pm2_5"],
        co=data["hourly"]["carbon_monoxide"],
        no2=data["hourly"]["nitrogen_dioxide"],
        so2=data["hourly"]["sulphur_dioxide"],
        o3=data["hourly"]["ozone"],
    )
    return result

